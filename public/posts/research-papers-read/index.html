<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Paper Read - My New Hugo Site</title><meta name="Description" content="Distilling concepts"><meta property="og:title" content="Paper Read" />
<meta property="og:description" content="The blog contains a summary of each research paper I have recently read and found interesting.
LLM as optimisers (Google) They propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. They show how linear regression and travelling salesman problem can be framed using text and the LLM does a decent job at arriving at the solution." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/posts/research-papers-read/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-23T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-09-23T00:00:00+00:00" /><meta property="og:site_name" content="MyShtick" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Paper Read"/>
<meta name="twitter:description" content="The blog contains a summary of each research paper I have recently read and found interesting.
LLM as optimisers (Google) They propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. They show how linear regression and travelling salesman problem can be framed using text and the LLM does a decent job at arriving at the solution."/>
<meta name="application-name" content="MyShtick">
<meta name="apple-mobile-web-app-title" content="MyShtick"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://example.org/posts/research-papers-read/" /><link rel="prev" href="http://example.org/posts/mandalorian/" /><link rel="next" href="http://example.org/posts/good-llm-intuitions/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Paper Read",
        "inLanguage": "en-us",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/example.org\/posts\/research-papers-read\/"
        },"genre": "posts","keywords": "papers","wordcount":  524 ,
        "url": "http:\/\/example.org\/posts\/research-papers-read\/","datePublished": "2023-09-23T00:00:00+00:00","dateModified": "2023-09-23T00:00:00+00:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "Nitin"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="My New Hugo Site">MyShtick</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="My New Hugo Site">MyShtick</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Paper Read</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>Nitin</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2023-09-23">2023-09-23</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;524 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;3 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#llm-as-optimisers-google">LLM as optimisers (Google)</a></li>
    <li><a href="#generative-agents-interactive-simulacra-of-human-behavior-google">Generative Agents: Interactive Simulacra of Human Behavior (Google)</a></li>
    <li><a href="#from-sparse-to-dense-gpt-4-summarization-with-chain-of-density-prompting-multiple">From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting (Multiple)</a></li>
    <li><a href="#chain-of-verification-reduces-hallucination-in-large-language-models-meta">Chain-Of-VErification reduces hallucination in large language models (Meta)</a></li>
    <li><a href="#think-before-you-speak-training-language-modelswith-pause-tokens-google">Think before you speak: Training Language ModelsWith Pause Tokens (Google)</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p>The blog contains a summary of each research paper I have recently read and found interesting.</p>
<h2 id="llm-as-optimisers-google">LLM as optimisers (Google)</h2>
<p>They propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. They show how linear regression and travelling salesman problem can be framed using text and the LLM does a decent job at arriving at the solution. But the most interesting part for me is covered under a section on prompt optimisation. Optimising the prompt i.e, finding the best prompt to get the desired response would be a boon for prompt engineers.
1. Paper - <a href="https://arxiv.org/abs/2309.03409" target="_blank" rel="noopener noreffer ">https://arxiv.org/abs/2309.03409</a>
2. Code - Not mentioned
3. Benchmark data - GSM8k and BigBench Hard</p>
<h2 id="generative-agents-interactive-simulacra-of-human-behavior-google">Generative Agents: Interactive Simulacra of Human Behavior (Google)</h2>
<p>They create 25 agents in a sandbox environment, akins to The Sims game, who interact with each other, simulating a real world interactions. And it all powered by gpt3.5-turbo. One aspect that stands out is the agent architecture which is designed to pick relevant pieces from the agent’s memory (which can be very long) and use them to inform agent behavior. I think this could be a solution (or at least offer some ideas) for the maintaining the relevant chat context problem for multi-turn conversations.
1. Paper - <a href="https://arxiv.org/abs/2304.03442" target="_blank" rel="noopener noreffer ">https://arxiv.org/abs/2304.03442</a>
2. Code - <a href="https://github.com/joonspk-research/generative_agents" target="_blank" rel="noopener noreffer ">https://github.com/joonspk-research/generative_agents</a>
3. Benchmark data - NA</p>
<h2 id="from-sparse-to-dense-gpt-4-summarization-with-chain-of-density-prompting-multiple">From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting (Multiple)</h2>
<p>From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting. They talk about packing more information into the summary by using this Chain of Density approach - recursively add entities without increasing summary length. Density is entities per token across the whole summary. Interesting prompting and evaluation methodology that could be useful for different summarisation requirements.
1. Paper - <a href="https://arxiv.org/pdf/2309.04269" target="_blank" rel="noopener noreffer ">https://arxiv.org/pdf/2309.04269</a>
2. Code - Not available
3. Benchmark data - <a href="https://huggingface.co/datasets/griffin/chain_of_density" target="_blank" rel="noopener noreffer ">https://huggingface.co/datasets/griffin/chain_of_density</a></p>
<h2 id="chain-of-verification-reduces-hallucination-in-large-language-models-meta">Chain-Of-VErification reduces hallucination in large language models (Meta)</h2>
<p>Paper that describes a methodology for reducing hallucinations in LLM models - specifically LLAMA 2 models. Reduces longform hallucinations via LLM double-checking its own work with shortform questions. The Chain-of-Verification (CoVe) recipe:
1. Generate baseline response
2. Generate a verification plan (set of questions)
3. Execute plan (answer questions)
4. Generate Final Verified Response (using answers)
Could be a useful recipe for reducing, if not eliminating hallucination, for other LLM models.
1. Paper - <a href="https://arxiv.org/pdf/2309.11495.pdf" target="_blank" rel="noopener noreffer ">https://arxiv.org/pdf/2309.11495.pdf</a>
2. Code - Not available
3. Benchmark data - WIKIDATA, WIKI-CATEGORY LIST, MULTISPANQA</p>
<h2 id="think-before-you-speak-training-language-modelswith-pause-tokens-google">Think before you speak: Training Language ModelsWith Pause Tokens (Google)</h2>
<p>In this study they try to use special <code>&lt;pause&gt;</code>tokens at the time of both training and inference. The idea is to use those pause tokens as a way to delay the response generation from the LLM model - allowing it more time to think and hence generate better responses. They empirically evaluate pause-training on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. They see good improvement. Interesting idea for future.
1. Paper - <a href="https://arxiv.org/pdf/2310.02226.pdf" target="_blank" rel="noopener noreffer ">https://arxiv.org/pdf/2310.02226.pdf</a>
2. Code - Not available
3. Benchmark data - GSM8k, SQuAD, CoQA etc</p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2023-09-23</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="http://example.org/posts/research-papers-read/" data-title="Paper Read" data-hashtags="papers"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="http://example.org/posts/research-papers-read/" data-hashtag="papers"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="http://example.org/posts/research-papers-read/"><i class="fab fa-linkedin fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on WhatsApp" data-sharer="whatsapp" data-url="http://example.org/posts/research-papers-read/" data-title="Paper Read" data-web><i class="fab fa-whatsapp fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/papers/">papers</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/mandalorian/" class="prev" rel="prev" title="What is great about Mandalorian Season 2"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>What is great about Mandalorian Season 2</a>
            <a href="/posts/good-llm-intuitions/" class="next" rel="next" title="Good intuitions about LLMs">Good intuitions about LLMs<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.117.0">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank"></a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":50},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
