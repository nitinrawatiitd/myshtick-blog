<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>llm - Tag - My New Hugo Site</title>
        <link>http://example.org/tags/llm/</link>
        <description>llm - Tag - My New Hugo Site</description>
        <generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 23 Sep 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="http://example.org/tags/llm/" rel="self" type="application/rss+xml" /><item>
    <title>Good intuitions about LLMs</title>
    <link>http://example.org/posts/good-llm-intuitions/</link>
    <pubDate>Sat, 23 Sep 2023 00:00:00 &#43;0000</pubDate>
    <author>Nitin</author>
    <guid>http://example.org/posts/good-llm-intuitions/</guid>
    <description><![CDATA[These few intuitions about LLMs at scale and training are helpful
Note: These notes are from presentation by Hyung Won Chung, Research Scientist at OpenAI.
talk: https://youtu.be/dbo3kNKPaUA?feature=shared slides: https://docs.google.com/presentation/d/1636wKStYdT_yRPbJNrf8MLKpQghuWGDmyHinHhAKeXY/edit#slide=id.g2885e521b53_0_0 Some abilities emerge with scale It has been observed that some capabilities only appear after a certain point in scale. Emergent abilities in large language models (Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph et al. (2022)) In gen ai space, it could appear that an idea doesn&rsquo;t work, but that could be just it doesn&rsquo;t work &ldquo;yet&rdquo;.]]></description>
</item>
<item>
    <title>Detecting machine generated text</title>
    <link>http://example.org/posts/detect-llm/</link>
    <pubDate>Wed, 30 Aug 2023 00:00:00 &#43;0000</pubDate>
    <author>Nitin</author>
    <guid>http://example.org/posts/detect-llm/</guid>
    <description><![CDATA[The adoption of LLM models and the output generated by them is going to proliferate the data that we are going to consume in the future. Now that is not necessary alarming, but sometimes, there can be a need to understand what is generate by a machine vs what is generated by a human. This blog covers some of the methods that can help there
Why you might need it? Regulation is a term that is thrown around a lot in the data science community to try &lsquo;rein in&rsquo; the dangers posed by generative AI models.]]></description>
</item>
<item>
    <title>Attention and Transformers</title>
    <link>http://example.org/posts/attention-and-transformers/</link>
    <pubDate>Thu, 24 Aug 2023 00:00:00 &#43;0000</pubDate>
    <author>Nitin</author>
    <guid>http://example.org/posts/attention-and-transformers/</guid>
    <description><![CDATA[Attention is all you Need was a landmark paper that changed the face of the NLP landscape. They are what power todays large scale LLMs - that have generated all the hype, most of which is merited.
This post is meant to develop a deeper understanding of the underlying concepts of the attention mechanism and the transformer architecture.
Queries, Keys, and Values Source: https://d2l.ai/chapter_attention-mechanisms-and-transformers
Simply consider the following: denote by $\mathcal{D} \stackrel{\textrm{def}}{=} {(\mathbf{k}_1, \mathbf{v}_1), \ldots (\mathbf{k}_m, \mathbf{v}_m)}$ a database of $m$ tuples of keys and values.]]></description>
</item>
</channel>
</rss>
